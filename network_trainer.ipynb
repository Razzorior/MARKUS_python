{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-05-29T19:02:29.441155Z",
     "start_time": "2024-05-29T19:02:29.434935Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available\n"
     ]
    }
   ],
   "source": [
    "# Import essential libraries\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import json\n",
    "import pickle\n",
    "import sys\n",
    "import tensorflow.keras as keras\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from tensorflow.keras import backend as K\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from custom_objects import custom_dead_relu_initializer\n",
    "\n",
    "gpu = len(tf.config.list_physical_devices('GPU')) > 0\n",
    "print(\"GPU is\", \"available\" if gpu else \"NOT AVAILABLE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "# Load Data\n",
    "(x_train_unnormalized, y_train), (x_test_unnormalized, y_test) = keras.datasets.mnist.load_data()\n",
    "x_train = x_train_unnormalized / 255.0\n",
    "x_test = x_test_unnormalized / 255.0\n",
    "# Inserting error:\n",
    "#y_train = np.where(y_train == 7, 1, y_train)\n",
    "dataset = (tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(64)).batch(32)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-02T09:12:54.631839Z",
     "start_time": "2024-05-02T09:12:54.437487Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "# Ideas for inserting errors:\n",
    "# Manipulate Class so that it results in error\n",
    "# Cause Exploding / Vanishing Gradients.\n",
    "# Look at adverserial attacks\n",
    "# Look at Dropout"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-30T10:23:29.714059Z",
     "start_time": "2024-04-30T10:23:29.700262Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_5 (Flatten)         (None, 784)               0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 200)               157000    \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 100)               20100     \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 10)                1010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 178,110\n",
      "Trainable params: 178,110\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential()\n",
    "model.add(keras.Input(shape=(28,28,)))\n",
    "model.add(keras.layers.Flatten())\n",
    "model.add(keras.layers.Dense(200, activation='relu', kernel_initializer=custom_dead_relu_initializer))\n",
    "model.add(keras.layers.Dense(100, activation='relu', kernel_initializer=custom_dead_relu_initializer))\n",
    "model.add(keras.layers.Dense(10))\n",
    "\n",
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-30T10:23:30.330665Z",
     "start_time": "2024-04-30T10:23:30.208841Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "# Apply loss function and optimizer, then compile the model\n",
    "loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "optimizer = keras.optimizers.Adam()\n",
    "model.compile(optimizer=optimizer, loss=loss_fn, metrics=[\"acc\"])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-30T10:23:32.061583Z",
     "start_time": "2024-04-30T10:23:32.021134Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.8893 - acc: 0.8148 - val_loss: 0.3177 - val_acc: 0.9103\n",
      "Epoch 2/10\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.3112 - acc: 0.9117 - val_loss: 0.2706 - val_acc: 0.9221\n",
      "Epoch 3/10\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.2071 - acc: 0.9400 - val_loss: 0.1602 - val_acc: 0.9517\n",
      "Epoch 4/10\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.1361 - acc: 0.9595 - val_loss: 0.1239 - val_acc: 0.9623\n",
      "Epoch 5/10\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.0999 - acc: 0.9694 - val_loss: 0.1406 - val_acc: 0.9582\n",
      "Epoch 6/10\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.0794 - acc: 0.9755 - val_loss: 0.0963 - val_acc: 0.9701\n",
      "Epoch 7/10\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.0625 - acc: 0.9803 - val_loss: 0.0911 - val_acc: 0.9737\n",
      "Epoch 8/10\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.0487 - acc: 0.9844 - val_loss: 0.0860 - val_acc: 0.9746\n",
      "Epoch 9/10\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.0403 - acc: 0.9869 - val_loss: 0.0982 - val_acc: 0.9755\n",
      "Epoch 10/10\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.0336 - acc: 0.9893 - val_loss: 0.1063 - val_acc: 0.9750\n"
     ]
    },
    {
     "data": {
      "text/plain": "<keras.callbacks.History at 0x1c612fad4f0>"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model on dataset for given epochs\n",
    "model.fit(x_train, y_train, batch_size=32, epochs=10, validation_data=(x_test, y_test))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-30T10:25:00.462768Z",
     "start_time": "2024-04-30T10:23:33.077179Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anzahl der \"toten\" Neuronen: 0\n"
     ]
    }
   ],
   "source": [
    "#first_layer_weights = model.layers[3].get_weights()[0]\n",
    "#dead_neurons = np.sum(np.all(first_layer_weights <= 0, axis=0))\n",
    "#print(f'Anzahl der \"toten\" Neuronen: {dead_neurons}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "# Save the model at the correct path\n",
    "name = \"two_layer_mlp_relu_dead\"\n",
    "file_name = name + \".keras\"\n",
    "model.save('saved_models/' + file_name)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-30T10:25:39.395733Z",
     "start_time": "2024-04-30T10:25:39.339867Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting with calculations\n",
      "Currently at batch: 0\n",
      "Currently at batch: 1\n",
      "Currently at batch: 2\n",
      "Currently at batch: 3\n",
      "Currently at batch: 4\n",
      "Currently at batch: 5\n",
      "Currently at batch: 6\n",
      "Currently at batch: 7\n",
      "Currently at batch: 8\n",
      "Currently at batch: 9\n",
      "Currently at batch: 10\n",
      "Currently at batch: 11\n",
      "Currently at batch: 12\n",
      "Currently at batch: 13\n",
      "Currently at batch: 14\n",
      "Currently at batch: 15\n",
      "Currently at batch: 16\n",
      "Currently at batch: 17\n",
      "Done with calculations. Saving to file.\n",
      "Starting with calculations\n",
      "Currently at batch: 0\n",
      "Currently at batch: 1\n",
      "Currently at batch: 2\n",
      "Currently at batch: 3\n",
      "Currently at batch: 4\n",
      "Currently at batch: 5\n",
      "Currently at batch: 6\n",
      "Currently at batch: 7\n",
      "Currently at batch: 8\n",
      "Currently at batch: 9\n",
      "Currently at batch: 10\n",
      "Currently at batch: 11\n",
      "Currently at batch: 12\n",
      "Currently at batch: 13\n",
      "Currently at batch: 14\n",
      "Currently at batch: 15\n",
      "Currently at batch: 16\n",
      "Currently at batch: 17\n",
      "Done with calculations. Saving to file.\n",
      "Starting with calculations\n",
      "Currently at batch: 0\n",
      "Currently at batch: 1\n",
      "Currently at batch: 2\n",
      "Currently at batch: 3\n",
      "Currently at batch: 4\n",
      "Currently at batch: 5\n",
      "Currently at batch: 6\n",
      "Currently at batch: 7\n",
      "Currently at batch: 8\n",
      "Currently at batch: 9\n",
      "Currently at batch: 10\n",
      "Currently at batch: 11\n",
      "Currently at batch: 12\n",
      "Currently at batch: 13\n",
      "Currently at batch: 14\n",
      "Currently at batch: 15\n",
      "Currently at batch: 16\n",
      "Currently at batch: 17\n",
      "Done with calculations. Saving to file.\n",
      "Done with class 0..\n",
      "Done with class 1..\n",
      "Done with class 2..\n",
      "Done with class 3..\n",
      "Done with class 4..\n",
      "Done with class 5..\n",
      "Done with class 6..\n",
      "Done with class 7..\n",
      "Done with class 8..\n",
      "Done with class 9..\n",
      "Computing overall activation averages..\n",
      "Averaging activation classes..\n",
      "Computing overall signal averages..\n",
      "Averaging signal classes..\n",
      "Saving results..\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "create_precalculations(name)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-02T14:10:29.913472Z",
     "start_time": "2024-05-02T14:10:29.226480Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "# Add functions that need to be recomputed after every new model (precalculations)\n",
    "# Add automatic creation of folder\n",
    "def create_precalculations(model_name):\n",
    "    model = keras.models.load_model('saved_models/' + model_name + '.keras')\n",
    "\n",
    "    # Create Folder for precalculations, if it doesn't exist yet.\n",
    "    save_file_path = 'saved_precalculations/' + model_name\n",
    "    try:\n",
    "        os.mkdir(save_file_path)\n",
    "    except OSError as error:\n",
    "        print(error)\n",
    "\n",
    "    create_subset(model, 'saved_precalculations/' + model_name + '/subset_activations_1.pickle')\n",
    "    create_subset(model, 'saved_precalculations/' + model_name + '/subset_activations_2.pickle')\n",
    "    create_subset(model, 'saved_precalculations/' + model_name + '/subset_activations_3.pickle')\n",
    "\n",
    "    calc_class_average_activation_and_signals(model, model_name)\n",
    "\n",
    "    print(\"Done\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-02T14:10:27.072042Z",
     "start_time": "2024-05-02T14:10:27.016173Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# TODO: Shift these functions to a seperate .py file and import them at the beginning of the nb\n",
    "def create_subset(model, file_path=None):\n",
    "    dataset = (tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(64)).batch(32)\n",
    "\n",
    "    dataset_size = sum(1 for _ in dataset)\n",
    "    one_percent_size = dataset_size // 100\n",
    "\n",
    "    # Take the first 1% of the dataset\n",
    "    one_percent_dataset = dataset.take(one_percent_size)\n",
    "\n",
    "    functions = []\n",
    "    results = []\n",
    "    for i in range(len(model.layers)):\n",
    "        functions.append(K.function([model.layers[i].input],[model.layers[i].output]))\n",
    "\n",
    "    print(\"Starting with calculations\")\n",
    "    for i, (batch, y) in enumerate(one_percent_dataset):\n",
    "        print(\"Currently at batch: \" + str(i))\n",
    "\n",
    "        output = functions[0](batch)\n",
    "        if i == 0:\n",
    "            results.append(np.transpose(np.squeeze(output, axis=0)))\n",
    "        else:\n",
    "            results[0] = np.concatenate([results[0], np.transpose(np.squeeze(output, axis=0))], axis=1)\n",
    "\n",
    "        for j in range(1, len(functions)):\n",
    "            tmp = functions[j](output)\n",
    "            output = tmp\n",
    "            if i == 0:\n",
    "                results.append(np.transpose(np.squeeze(output, axis=0)))\n",
    "            else:\n",
    "                results[j] = np.concatenate([results[j], np.transpose(np.squeeze(output, axis=0))], axis=1)\n",
    "\n",
    "    print(\"Done with calculations. Saving to file.\")\n",
    "\n",
    "    with open(file_path, 'wb') as handle:\n",
    "        pickle.dump(results, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-02T14:09:02.762527Z",
     "start_time": "2024-05-02T14:09:02.746604Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "def calc_average_activation_and_signals(model, file_path_activation, file_path_signals):\n",
    "    dataset = (tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(64)).batch(32, drop_remainder=True)\n",
    "\n",
    "    functions = []\n",
    "    transposed_weights = []\n",
    "    for i in range(len(model.layers)):\n",
    "        functions.append(K.function([model.layers[i].input],model.layers[i].output))\n",
    "        if len(model.layers[i].get_weights()) == 0:\n",
    "            transposed_weights.append([])\n",
    "        else:\n",
    "            transposed_weights.append(tf.transpose(model.layers[i].get_weights()[0]))\n",
    "\n",
    "    average_signals_per_layer = []\n",
    "    average_activations_per_layer = []\n",
    "    print(\"Starting with calculations\")\n",
    "    for i, (batch, y) in enumerate(dataset):\n",
    "        print(\"Currently at batch: \" + str(i))\n",
    "\n",
    "        output = functions[0](batch)\n",
    "        if i == 0:\n",
    "            average_activations_per_layer.append(tf.expand_dims(tf.reduce_mean(output, axis=0), axis=0))\n",
    "        else:\n",
    "            average_activations_per_layer[0] = np.concatenate([average_activations_per_layer[0], tf.expand_dims(tf.reduce_mean(output, axis=0), axis=0)], axis=0)\n",
    "\n",
    "        for j in range(1, len(functions)):\n",
    "            output = functions[j](output)\n",
    "            if i == 0:\n",
    "                average_activations_per_layer.append(tf.expand_dims(tf.reduce_mean(output, axis=0), axis=0))\n",
    "                average_signals_per_layer.append(tf.expand_dims(tf.reduce_mean(tf.einsum('bi,ij->bji', output, transposed_weights[j]),axis=0), axis=0))\n",
    "            else:\n",
    "                average_activations_per_layer[j] = np.concatenate([average_activations_per_layer[j], tf.expand_dims(tf.reduce_mean(output, axis=0), axis=0)], axis=0)\n",
    "                average_signals_per_layer[j-1] = np.concatenate([average_signals_per_layer[j-1], tf.expand_dims(tf.reduce_mean(tf.einsum('bi,ij->bji', output, transposed_weights[j]),axis=0), axis=0)],axis=0)\n",
    "\n",
    "    print(\"Done with calculations, now averaging results..\")\n",
    "\n",
    "    for index in range(len(average_activations_per_layer)):\n",
    "        average_activations_per_layer[index] = tf.reduce_mean(average_activations_per_layer[index], axis=0).numpy()\n",
    "\n",
    "    for index in range(len(average_signals_per_layer)):\n",
    "        average_signals_per_layer[index] = tf.reduce_mean(average_signals_per_layer[index], axis=0).numpy()\n",
    "\n",
    "    print(\"Done with calculations. Saving to file.\")\n",
    "\n",
    "    with open(file_path_activation, 'wb') as handle:\n",
    "        pickle.dump(average_activations_per_layer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    with open(file_path_signals, 'wb') as handle:\n",
    "        pickle.dump(average_signals_per_layer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    return"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "def calc_average_signal(model, file_path):\n",
    "    dataset = (tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(64)).batch(32)\n",
    "    summed_signal_per_layer = []\n",
    "\n",
    "    print(\"Starting with calculations\")\n",
    "    for i, (batch, _) in enumerate(dataset):\n",
    "        print(\"Currently at batch: \" + str(i))\n",
    "        inputs = batch\n",
    "        for j in range(len(model.layers)-1):\n",
    "            # Compute the activations of the current layer\n",
    "            activations = model.layers[j](inputs)\n",
    "            # Get the weights and biases of the current layer\n",
    "            weights, biases = model.layers[j+1].get_weights()\n",
    "            # Compute the outgoing signals (weight times activation)\n",
    "            expanded_activations = np.expand_dims(activations, axis=-1)\n",
    "            expanded_weights = np.expand_dims(weights, axis=0)\n",
    "            outgoing_signals = expanded_activations * expanded_weights\n",
    "            # Store the outgoing signals for the current layer\n",
    "            if i == 0:\n",
    "                summed_signal_per_layer.append(tf.reduce_sum(outgoing_signals, axis=0))\n",
    "            else:\n",
    "                summed_signal_per_layer[j] += tf.reduce_sum(outgoing_signals, axis=0)\n",
    "            # Update inputs for the next layer\n",
    "            inputs = activations\n",
    "\n",
    "\n",
    "    print(\"Done with calculations, averaging results..\")\n",
    "    average_signal_per_layer = []\n",
    "    for matrix in summed_signal_per_layer:\n",
    "        average_signal_per_layer.append((matrix / x_train.shape[0]).numpy())\n",
    "    with open(file_path, 'wb') as handle:\n",
    "        pickle.dump(average_signal_per_layer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    return"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "def calc_class_average_activation_and_signals(model, model_name):\n",
    "    (x_train_unnormalized, y_train), (x_test_unnormalized, y_test) = keras.datasets.mnist.load_data()\n",
    "    x_train = x_train_unnormalized / 255.0\n",
    "    x_test = x_test_unnormalized / 255.0\n",
    "    batch_size = 32\n",
    "\n",
    "    functions = []\n",
    "    weights = []\n",
    "\n",
    "    class_average_signals_per_layer = []\n",
    "    class_average_activations_per_layer = []\n",
    "\n",
    "    for i in range(len(model.layers)):\n",
    "        functions.append(K.function([model.layers[i].input],model.layers[i].output))\n",
    "        if len(model.layers[i].get_weights()) > 0:\n",
    "            weights.append(model.layers[i].get_weights()[0])\n",
    "\n",
    "    # 10 is hardcoded for the amount of MNIST classes. In case there is a different model we need to change this part (and the dataloading of course)\n",
    "    for class_index in range (10):\n",
    "        class_images = x_train[y_train==class_index]\n",
    "        dataset = tf.data.Dataset.from_tensor_slices(class_images).batch(batch_size)\n",
    "        class_activations = []\n",
    "        class_signals = []\n",
    "        for i, batch in enumerate(dataset):\n",
    "            output = functions[0](batch)\n",
    "            if i == 0:\n",
    "                class_activations.append(tf.expand_dims(tf.reduce_sum(output, axis=0), axis=0))\n",
    "                class_signals.append(tf.expand_dims(tf.reduce_sum(tf.einsum('bi,ij->bij', output, weights[0]),axis=0), axis=0))\n",
    "            else:\n",
    "                class_activations[0] = tf.add(class_activations[0], tf.expand_dims(tf.reduce_sum(output, axis=0), axis=0))\n",
    "                class_signals[0] = tf.add(class_signals[0], tf.expand_dims(tf.reduce_sum(tf.einsum('bi,ij->bij', output, weights[0]),axis=0), axis=0))\n",
    "\n",
    "            for j in range(1, len(functions)-1):\n",
    "                output = functions[j](output)\n",
    "                if i == 0:\n",
    "                    class_activations.append(tf.expand_dims(tf.reduce_sum(output, axis=0), axis=0))\n",
    "                    class_signals.append(tf.expand_dims(tf.reduce_sum(tf.einsum('bi,ij->bij', output, weights[j]),axis=0), axis=0))\n",
    "                else:\n",
    "                    class_activations[j] = tf.add(class_activations[j], tf.expand_dims(tf.reduce_sum(output, axis=0), axis=0))\n",
    "                    class_signals[j] = tf.add(class_signals[j], tf.expand_dims(tf.reduce_sum(tf.einsum('bi,ij->bij', output, weights[j]),axis=0), axis=0))\n",
    "\n",
    "            output = functions[len(functions)-1](output)\n",
    "            if i == 0:\n",
    "                class_activations.append(tf.expand_dims(tf.reduce_sum(output, axis=0), axis=0))\n",
    "            else:\n",
    "                class_activations[len(functions)-1] = tf.add(class_activations[len(functions)-1], tf.expand_dims(tf.reduce_sum(output, axis=0), axis=0))\n",
    "\n",
    "        print(\"Done with class {}..\".format(str(class_index)))\n",
    "\n",
    "        class_average_activations_per_layer.append(class_activations)\n",
    "        class_average_signals_per_layer.append(class_signals)\n",
    "\n",
    "    print(\"Computing overall activation averages..\")\n",
    "    average_activations_per_layer = []\n",
    "    num_layers = len(class_average_activations_per_layer[0])\n",
    "    for layer_index in range(num_layers):\n",
    "        # Initialisieren Sie einen Tensor, um die summierten Aktivierungen für die aktuelle Schicht zu speichern\n",
    "        summed_activations = tf.zeros_like(class_average_activations_per_layer[0][layer_index])\n",
    "\n",
    "        # Addieren Sie die durchschnittlichen Aktivierungen der aktuellen Schicht über alle Klassen hinweg\n",
    "        for class_activations in class_average_activations_per_layer:\n",
    "            summed_activations = tf.add(summed_activations, class_activations[layer_index])\n",
    "\n",
    "        # Fügen Sie den Durchschnitt der aktuellen Schicht zur Ergebnisliste hinzu\n",
    "        average_activations_per_layer.append(summed_activations)\n",
    "\n",
    "    for layer_index in range(num_layers):\n",
    "        average_activations_per_layer[layer_index] = np.squeeze(tf.divide(average_activations_per_layer[layer_index], x_train.shape[0]).numpy())\n",
    "\n",
    "    print(\"Averaging activation classes..\")\n",
    "    for class_index, class_activations in enumerate(class_average_activations_per_layer):\n",
    "        for layer_index, layer in enumerate(class_activations):\n",
    "            class_average_activations_per_layer[class_index][layer_index] = np.squeeze((tf.divide(layer[0], np.sum(y_train == class_index))).numpy())\n",
    "\n",
    "    print(\"Computing overall signal averages..\")\n",
    "    average_signals_per_layer = []\n",
    "    num_layers = len(class_average_signals_per_layer[0])\n",
    "    for layer_index in range(num_layers):\n",
    "        summed_activations = tf.zeros_like(class_average_signals_per_layer[0][layer_index])\n",
    "\n",
    "        for class_activations in class_average_signals_per_layer:\n",
    "            summed_activations = tf.add(summed_activations, class_activations[layer_index])\n",
    "\n",
    "        average_signals_per_layer.append(summed_activations)\n",
    "\n",
    "    for layer_index in range(num_layers):\n",
    "        average_signals_per_layer[layer_index] = np.squeeze(tf.divide(average_signals_per_layer[layer_index], x_train.shape[0]).numpy())\n",
    "\n",
    "    print(\"Averaging signal classes..\")\n",
    "    for class_index, class_signals in enumerate(class_average_signals_per_layer):\n",
    "        for layer_index, layer in enumerate(class_signals):\n",
    "            class_average_signals_per_layer[class_index][layer_index] = np.squeeze(tf.divide(layer[0], np.sum(y_train == class_index)).numpy())\n",
    "\n",
    "    print(\"Saving results..\")\n",
    "    file_path = 'saved_precalculations/' + model_name + '/average_activations.pickle'\n",
    "    with open(file_path, 'wb') as handle:\n",
    "        pickle.dump(average_activations_per_layer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    file_path = 'saved_precalculations/' + model_name + '/average_signals.pickle'\n",
    "    with open(file_path, 'wb') as handle:\n",
    "        pickle.dump(average_signals_per_layer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    file_path = 'saved_precalculations/' + model_name + '/class_average_activations.pickle'\n",
    "    with open(file_path, 'wb') as handle:\n",
    "        pickle.dump(class_average_activations_per_layer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    file_path = 'saved_precalculations/' + model_name + '/class_average_signals.pickle'\n",
    "    with open(file_path, 'wb') as handle:\n",
    "        pickle.dump(class_average_signals_per_layer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    return"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
